import re
import time
import gensim
from rake_nltk import Rake

# import BM25
# import Scoring
from BM25 import entry
from Scoring import wordLevel_QC_relatedness
from Scoring import dateExtractor


if __name__ == '__main__':
    length_BM25 = 10
    path_Query = './data/10_query_list.txt'
    path_Corpus = './data/1K_scotus_corpus.txt'
    # path_Corpus = './data/all_scotus_corpus.txt'
    # path_Corpus = './data/test_scotus_corpus.txt'

    queries, dict_BM25 = entry.main_BM25(path_Query, path_Corpus, length_BM25)
    print('queries: ', queries)
    print('dictionary for result after BM25: ', dict_BM25)

    # here we load word2vec model to save loading time. Will be change in the future.
    pretrained_embedding_path = "../word2vec/GoogleNews-vectors-negative300.bin"
    tic = time.time()
    print('Please wait ... (it could take a while to load the file : {})'.format(pretrained_embedding_path))
    model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embedding_path, binary=True)
    print('Done.  (time used: {:.1f}s)\n'.format(time.time()-tic))
    # embedding_dim = 300

    # initialize for Key Phrases Overlap (second part of Scoring)
    rake_object = Rake()

    corpus_dir = './data/all_scotus_text/'
    date_dir = './data/all_scotus_NYU_IE1/'
    # for i in range(len(queries)):
    for i in range(1): # for development setting
        print('============================== Query{}: {} =================================='.format(i+1, queries[i]))
        each_query_result_list = dict_BM25.get(i, 0)
        # print(each_query_result_list) # docs ID
        embedding_each_query = wordLevel_QC_relatedness.get_embeddings_from_pretrained_googlenews_w2v(queries[i], model)
        print()
        # print('length of query embedding: ', len(embedding_each_query))

        docs_date_list = []
        for j in range(len(each_query_result_list)):
            file_index = str(each_query_result_list[j])
            file_name = corpus_dir + file_index + '.txt'
            # file_name = corpus_dir + str(each_query_result_list[j]) + '.txt'

            # Date Part
            # date_file_name = date_dir + str(each_query_result_list[j]) + '.NYU_IE1'

            date_of_each_file = dateExtractor.date_extract_from_single_file(date_dir, file_index)
            # if date_of_each_file != -1:
            docs_date_list.append([date_of_each_file,file_index])
        sort_docs_date_list = sorted(docs_date_list, reverse=True)
        date_weight = 1
        date_decay_factor = 0.03
        for i in range(len(sort_docs_date_list)):
            date = sort_docs_date_list[i][0]
            if date != -1:
                sort_docs_date_list[i].append(date_weight)
                date_weight -= date_decay_factor
            else:
                sort_docs_date_list[i].append(0.85) # To deal with those file without date, we give an average score. Based on 10 extracted files, we give 0.85.
        print(sort_docs_date_list)







            # print(str(each_query_result_list[j]))

            # print('The score for {} in Scotus'.format(file_name.split('/')[3]))
            # with open(file_name, 'r') as f:
            #     doc_text = f.read().lower()
            #     each_doc_text = doc_text.split()
            #
            #     # word level QC relatedness score
            #     embedding_each_doc = wordLevel_QC_relatedness.get_embeddings_from_pretrained_googlenews_w2v(each_doc_text, model)
            #     # print('length of doc embedding: ', len(embedding_each_doc))
            #     score_word_level_QC = wordLevel_QC_relatedness.calc_wordLevel_QC_relatedness_score(embedding_each_query, embedding_each_doc)
            #     print('each doc word level QC relatedness score: ', score_word_level_QC)
            #
            #     # Key Phrases overlap between query and docs
            #     score_KP = 0
            #     rake_object.extract_keywords_from_text(doc_text)
            #     # Here we consider only top 20 results. and using the score generated by RAKE algorithm. decayed on each position with ratio 0.95.
            #     top_n_result_RAKE = 20
            #     result_list_RAKE = rake_object.get_ranked_phrases_with_scores()[:top_n_result_RAKE] # get_ranked_phrases() to ignore score.
            #     # print('Key Phrases extracted with Rake algorithm: ', result_list_RAKE)
            #     decay_factor = 1
            #     ratio_to_decay = 0.95
            #     combined_item_in_query = ''.join(queries[i])
            #     total_match_factor = 1.5
            #     for score_RAKE, KP_RAKE in result_list_RAKE:
            #         if combined_item_in_query in KP_RAKE:
            #             score_KP += total_match_factor * score_RAKE * decay_factor
            #         else:
            #             for item in queries[i]:
            #                 if item in KP_RAKE:
            #                     score_KP += score_RAKE * decay_factor
            #                     break # we only consider one match in query at most for now.
            #         decay_factor *= ratio_to_decay
            #     print('each doc Key Phrases overlap score: ', score_KP)
            #     print()
            #
            #     # Date Part








                # print('length of each doc text: ', len(each_doc_text))


                # tmp = []
                # for line in f.readlines():
                #     line = line.lower()
                #     # desc = line.strip().split('\t')
                #     # item = line.split()
                #     item = re.split('\W', line)
                #     tmp.extend(item)
                # print('tmp: ', len(tmp))





